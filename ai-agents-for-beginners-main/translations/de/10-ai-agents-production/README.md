<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8164484c16b1ed3287ef9dae9fc437c1",
  "translation_date": "2025-07-23T08:08:42+00:00",
  "source_file": "10-ai-agents-production/README.md",
  "language_code": "de"
}
-->
# KI-Agenten in der Produktion: Beobachtbarkeit & Bewertung

Wenn KI-Agenten von experimentellen Prototypen zu realen Anwendungen √ºbergehen, wird die F√§higkeit, ihr Verhalten zu verstehen, ihre Leistung zu √ºberwachen und ihre Ergebnisse systematisch zu bewerten, entscheidend.

## Lernziele

Nach Abschluss dieser Lektion werden Sie wissen/verstehen:
- Grundkonzepte der Beobachtbarkeit und Bewertung von Agenten
- Techniken zur Verbesserung der Leistung, Kosten und Effektivit√§t von Agenten
- Was und wie Sie Ihre KI-Agenten systematisch bewerten k√∂nnen
- Wie Sie die Kosten bei der Bereitstellung von KI-Agenten in der Produktion kontrollieren k√∂nnen
- Wie Sie Agenten, die mit AutoGen erstellt wurden, instrumentieren k√∂nnen

Das Ziel ist, Sie mit dem Wissen auszustatten, um Ihre "Black-Box"-Agenten in transparente, handhabbare und zuverl√§ssige Systeme zu verwandeln.

_**Hinweis:** Es ist wichtig, KI-Agenten bereitzustellen, die sicher und vertrauensw√ºrdig sind. Sehen Sie sich die Lektion [Vertrauensw√ºrdige KI-Agenten erstellen](./06-building-trustworthy-agents/README.md) an._

## Traces und Spans

Beobachtungswerkzeuge wie [Langfuse](https://langfuse.com/) oder [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-foundry/what-is-azure-ai-foundry) stellen Agentenl√§ufe normalerweise als Traces und Spans dar.

- **Trace** repr√§sentiert eine vollst√§ndige Agentenaufgabe von Anfang bis Ende (z. B. die Bearbeitung einer Benutzeranfrage).
- **Spans** sind einzelne Schritte innerhalb des Traces (z. B. der Aufruf eines Sprachmodells oder das Abrufen von Daten).

![Trace-Baum in Langfuse](https://langfuse.com/images/cookbook/example-autogen-evaluation/trace-tree.png)

Ohne Beobachtbarkeit kann sich ein KI-Agent wie eine "Black Box" anf√ºhlen ‚Äì sein interner Zustand und seine Argumentation sind undurchsichtig, was es schwierig macht, Probleme zu diagnostizieren oder die Leistung zu optimieren. Mit Beobachtbarkeit werden Agenten zu "Glasboxen", die Transparenz bieten, die entscheidend ist, um Vertrauen aufzubauen und sicherzustellen, dass sie wie beabsichtigt funktionieren.

## Warum Beobachtbarkeit in Produktionsumgebungen wichtig ist

Die Einf√ºhrung von KI-Agenten in Produktionsumgebungen bringt neue Herausforderungen und Anforderungen mit sich. Beobachtbarkeit ist nicht mehr nur ein "Nice-to-have", sondern eine kritische F√§higkeit:

*   **Fehlerbehebung und Ursachenanalyse**: Wenn ein Agent fehlschl√§gt oder eine unerwartete Ausgabe erzeugt, liefern Beobachtungswerkzeuge die Traces, die ben√∂tigt werden, um die Fehlerquelle zu identifizieren. Dies ist besonders wichtig bei komplexen Agenten, die mehrere LLM-Aufrufe, Werkzeuginteraktionen und bedingte Logik umfassen k√∂nnen.
*   **Latenz- und Kostenmanagement**: KI-Agenten verlassen sich oft auf LLMs und andere externe APIs, die pro Token oder pro Aufruf abgerechnet werden. Beobachtbarkeit erm√∂glicht eine pr√§zise Verfolgung dieser Aufrufe, um Operationen zu identifizieren, die √ºberm√§√üig langsam oder teuer sind. Dies hilft Teams, Prompts zu optimieren, effizientere Modelle auszuw√§hlen oder Workflows neu zu gestalten, um Betriebskosten zu verwalten und eine gute Benutzererfahrung zu gew√§hrleisten.
*   **Vertrauen, Sicherheit und Compliance**: In vielen Anwendungen ist es wichtig sicherzustellen, dass Agenten sicher und ethisch handeln. Beobachtbarkeit bietet eine Pr√ºfspur der Aktionen und Entscheidungen des Agenten. Dies kann verwendet werden, um Probleme wie Prompt Injection, die Generierung sch√§dlicher Inhalte oder den unsachgem√§√üen Umgang mit personenbezogenen Daten (PII) zu erkennen und zu beheben. Beispielsweise k√∂nnen Sie Traces √ºberpr√ºfen, um zu verstehen, warum ein Agent eine bestimmte Antwort gegeben oder ein bestimmtes Werkzeug verwendet hat.
*   **Kontinuierliche Verbesserungszyklen**: Beobachtungsdaten sind die Grundlage eines iterativen Entwicklungsprozesses. Durch die √úberwachung der Leistung von Agenten in der realen Welt k√∂nnen Teams Verbesserungsbereiche identifizieren, Daten f√ºr die Feinabstimmung von Modellen sammeln und die Auswirkungen von √Ñnderungen validieren. Dies schafft eine Feedback-Schleife, bei der Produktionsinformationen aus der Online-Bewertung Offline-Experimente und -Verfeinerungen informieren, was zu einer schrittweisen Verbesserung der Agentenleistung f√ºhrt.

## Wichtige Metriken, die verfolgt werden sollten

Um das Verhalten von Agenten zu √ºberwachen und zu verstehen, sollten eine Reihe von Metriken und Signalen verfolgt werden. W√§hrend die spezifischen Metriken je nach Zweck des Agenten variieren k√∂nnen, sind einige universell wichtig.

Hier sind einige der h√§ufigsten Metriken, die von Beobachtungswerkzeugen √ºberwacht werden:

**Latenz:** Wie schnell reagiert der Agent? Lange Wartezeiten wirken sich negativ auf die Benutzererfahrung aus. Sie sollten die Latenz f√ºr Aufgaben und einzelne Schritte durch die Verfolgung von Agentenl√§ufen messen. Beispielsweise k√∂nnte ein Agent, der 20 Sekunden f√ºr alle Modellaufrufe ben√∂tigt, durch die Verwendung eines schnelleren Modells oder durch paralleles Ausf√ºhren von Modellaufrufen beschleunigt werden.

**Kosten:** Was sind die Kosten pro Agentenlauf? KI-Agenten verlassen sich auf LLM-Aufrufe, die pro Token oder externe APIs abgerechnet werden. H√§ufige Werkzeugnutzung oder mehrere Prompts k√∂nnen die Kosten schnell erh√∂hen. Beispielsweise, wenn ein Agent ein LLM f√ºnfmal aufruft, um eine marginale Qualit√§tsverbesserung zu erzielen, m√ºssen Sie beurteilen, ob die Kosten gerechtfertigt sind oder ob Sie die Anzahl der Aufrufe reduzieren oder ein g√ºnstigeres Modell verwenden k√∂nnten. Echtzeit√ºberwachung kann auch unerwartete Spitzen identifizieren (z. B. Fehler, die √ºberm√§√üige API-Schleifen verursachen).

**Anfragefehler:** Wie viele Anfragen hat der Agent nicht erfolgreich bearbeitet? Dies kann API-Fehler oder fehlgeschlagene Werkzeugaufrufe umfassen. Um Ihren Agenten in der Produktion robuster gegen diese zu machen, k√∂nnen Sie Fallbacks oder Wiederholungen einrichten. Z. B. wenn Anbieter A f√ºr LLMs ausf√§llt, wechseln Sie zu Anbieter B als Backup.

**Benutzerfeedback:** Die Implementierung direkter Benutzerbewertungen liefert wertvolle Einblicke. Dies kann explizite Bewertungen (üëçDaumen hoch/üëérunter, ‚≠ê1-5 Sterne) oder Textkommentare umfassen. Konsistentes negatives Feedback sollte Sie alarmieren, da dies ein Zeichen daf√ºr ist, dass der Agent nicht wie erwartet funktioniert.

**Implizites Benutzerfeedback:** Benutzerverhalten liefert indirektes Feedback, auch ohne explizite Bewertungen. Dies kann das sofortige Umformulieren von Fragen, wiederholte Anfragen oder das Klicken auf eine Wiederholungsschaltfl√§che umfassen. Z. B. wenn Sie sehen, dass Benutzer wiederholt dieselbe Frage stellen, ist dies ein Zeichen daf√ºr, dass der Agent nicht wie erwartet funktioniert.

**Genauigkeit:** Wie oft erzeugt der Agent korrekte oder w√ºnschenswerte Ergebnisse? Genauigkeitsdefinitionen variieren (z. B. Probleml√∂sungsgenauigkeit, Informationsabrufgenauigkeit, Benutzerzufriedenheit). Der erste Schritt besteht darin, zu definieren, wie Erfolg f√ºr Ihren Agenten aussieht. Sie k√∂nnen die Genauigkeit √ºber automatisierte Pr√ºfungen, Bewertungsscores oder Aufgabenabschlusskennzeichnungen verfolgen. Beispielsweise k√∂nnen Traces als "erfolgreich" oder "fehlgeschlagen" markiert werden.

**Automatisierte Bewertungsmetriken:** Sie k√∂nnen auch automatisierte Bewertungen einrichten. Beispielsweise k√∂nnen Sie ein LLM verwenden, um die Ausgabe des Agenten zu bewerten, z. B. ob sie hilfreich, genau oder nicht ist. Es gibt auch mehrere Open-Source-Bibliotheken, die Ihnen helfen, verschiedene Aspekte des Agenten zu bewerten. Z. B. [RAGAS](https://docs.ragas.io/) f√ºr RAG-Agenten oder [LLM Guard](https://llm-guard.com/), um sch√§dliche Sprache oder Prompt Injection zu erkennen.

In der Praxis bietet eine Kombination dieser Metriken die beste Abdeckung der Gesundheit eines KI-Agenten. Im [Beispiel-Notebook](../../../10-ai-agents-production/code_samples/10_autogen_evaluation.ipynb) dieses Kapitels zeigen wir Ihnen, wie diese Metriken in realen Beispielen aussehen, aber zuerst lernen wir, wie ein typischer Bewertungsworkflow aussieht.

## Instrumentieren Sie Ihren Agenten

Um Tracing-Daten zu sammeln, m√ºssen Sie Ihren Code instrumentieren. Ziel ist es, den Agenten-Code so zu instrumentieren, dass Traces und Metriken ausgegeben werden, die von einer Beobachtungsplattform erfasst, verarbeitet und visualisiert werden k√∂nnen.

**OpenTelemetry (OTel):** [OpenTelemetry](https://opentelemetry.io/) hat sich als Industriestandard f√ºr LLM-Beobachtbarkeit etabliert. Es bietet eine Reihe von APIs, SDKs und Tools zum Generieren, Sammeln und Exportieren von Telemetriedaten.

Es gibt viele Instrumentierungsbibliotheken, die bestehende Agenten-Frameworks umschlie√üen und das Exportieren von OpenTelemetry-Spans zu einem Beobachtungswerkzeug erleichtern. Unten finden Sie ein Beispiel zur Instrumentierung eines AutoGen-Agenten mit der [OpenLit-Instrumentierungsbibliothek](https://github.com/openlit/openlit):

```python
import openlit

openlit.init(tracer = langfuse._otel_tracer, disable_batch = True)
```

Das [Beispiel-Notebook](../../../10-ai-agents-production/code_samples/10_autogen_evaluation.ipynb) in diesem Kapitel zeigt, wie Sie Ihren AutoGen-Agenten instrumentieren k√∂nnen.

**Manuelle Span-Erstellung:** W√§hrend Instrumentierungsbibliotheken eine gute Grundlage bieten, gibt es oft F√§lle, in denen detailliertere oder benutzerdefinierte Informationen ben√∂tigt werden. Sie k√∂nnen Spans manuell erstellen, um benutzerdefinierte Anwendungslogik hinzuzuf√ºgen. Noch wichtiger ist, dass sie automatisch oder manuell erstellte Spans mit benutzerdefinierten Attributen (auch bekannt als Tags oder Metadaten) anreichern k√∂nnen. Diese Attribute k√∂nnen gesch√§ftsspezifische Daten, Zwischenberechnungen oder jeden Kontext umfassen, der f√ºr Debugging oder Analyse n√ºtzlich sein k√∂nnte, wie `user_id`, `session_id` oder `model_version`.

Beispiel zur manuellen Erstellung von Traces und Spans mit dem [Langfuse Python SDK](https://langfuse.com/docs/sdk/python/sdk-v3):

```python
from langfuse import get_client
 
langfuse = get_client()
 
span = langfuse.start_span(name="my-span")
 
span.end()
```

## Agentenbewertung

Beobachtbarkeit liefert uns Metriken, aber die Bewertung ist der Prozess der Analyse dieser Daten (und Durchf√ºhrung von Tests), um festzustellen, wie gut ein KI-Agent funktioniert und wie er verbessert werden kann. Mit anderen Worten, sobald Sie diese Traces und Metriken haben, wie nutzen Sie sie, um den Agenten zu beurteilen und Entscheidungen zu treffen?

Regelm√§√üige Bewertung ist wichtig, da KI-Agenten oft nicht-deterministisch sind und sich entwickeln k√∂nnen (durch Updates oder driftendes Modellverhalten) ‚Äì ohne Bewertung w√ºrden Sie nicht wissen, ob Ihr "intelligenter Agent" tats√§chlich seine Aufgabe gut erf√ºllt oder ob er sich verschlechtert hat.

Es gibt zwei Kategorien von Bewertungen f√ºr KI-Agenten: **Online-Bewertung** und **Offline-Bewertung**. Beide sind wertvoll und erg√§nzen sich gegenseitig. Normalerweise beginnen wir mit der Offline-Bewertung, da dies der Mindestschritt ist, bevor ein Agent bereitgestellt wird.

### Offline-Bewertung

![Datensatz-Elemente in Langfuse](https://langfuse.com/images/cookbook/example-autogen-evaluation/example-dataset.png)

Dies beinhaltet die Bewertung des Agenten in einer kontrollierten Umgebung, typischerweise mit Testdatens√§tzen, nicht mit Live-Benutzeranfragen. Sie verwenden kuratierte Datens√§tze, bei denen Sie wissen, was die erwartete Ausgabe oder das korrekte Verhalten ist, und f√ºhren dann Ihren Agenten darauf aus.

Beispielsweise, wenn Sie einen Agenten f√ºr mathematische Textaufgaben erstellt haben, k√∂nnten Sie einen [Testdatensatz](https://huggingface.co/datasets/gsm8k) mit 100 Problemen und bekannten Antworten haben. Offline-Bewertung wird oft w√§hrend der Entwicklung durchgef√ºhrt (und kann Teil von CI/CD-Pipelines sein), um Verbesserungen zu √ºberpr√ºfen oder Regressionen zu verhindern. Der Vorteil ist, dass sie **wiederholbar ist und Sie klare Genauigkeitsmetriken erhalten k√∂nnen, da Sie eine Ground Truth haben**. Sie k√∂nnten auch Benutzeranfragen simulieren und die Antworten des Agenten mit idealen Antworten vergleichen oder automatisierte Metriken wie oben beschrieben verwenden.

Die Hauptherausforderung bei der Offline-Bewertung besteht darin, sicherzustellen, dass Ihr Testdatensatz umfassend und relevant bleibt ‚Äì der Agent k√∂nnte auf einem festen Testdatensatz gut abschneiden, aber sehr unterschiedliche Anfragen in der Produktion begegnen. Daher sollten Sie Testdatens√§tze mit neuen Randf√§llen und Beispielen aktualisieren, die reale Szenarien widerspiegeln‚Äã. Eine Mischung aus kleinen "Smoke-Test"-F√§llen und gr√∂√üeren Bewertungss√§tzen ist n√ºtzlich: kleine S√§tze f√ºr schnelle √úberpr√ºfungen und gr√∂√üere f√ºr umfassendere Leistungsmetriken‚Äã.

### Online-Bewertung

![√úbersicht √ºber Beobachtungsmetriken](https://langfuse.com/images/cookbook/example-autogen-evaluation/dashboard.png)

Dies bezieht sich auf die Bewertung des Agenten in einer Live-Umgebung, d. h. w√§hrend der tats√§chlichen Nutzung in der Produktion. Online-Bewertung beinhaltet die kontinuierliche √úberwachung der Leistung des Agenten bei realen Benutzerinteraktionen und die Analyse der Ergebnisse.

Beispielsweise k√∂nnten Sie Erfolgsraten, Benutzerzufriedenheitsbewertungen oder andere Metriken im Live-Verkehr verfolgen. Der Vorteil der Online-Bewertung ist, dass sie **Dinge erfasst, die Sie in einer Laboreinstellung m√∂glicherweise nicht vorhersehen** ‚Äì Sie k√∂nnen Modelldrift im Laufe der Zeit beobachten (wenn die Effektivit√§t des Agenten abnimmt, w√§hrend sich Eingabemuster √§ndern) und unerwartete Anfragen oder Situationen erfassen, die nicht in Ihren Testdaten enthalten waren‚Äã. Sie bietet ein echtes Bild davon, wie sich der Agent in der Praxis verh√§lt.

Online-Bewertung beinhaltet oft das Sammeln von implizitem und explizitem Benutzerfeedback, wie besprochen, und m√∂glicherweise das Durchf√ºhren von Shadow-Tests oder A/B-Tests (bei denen eine neue Version des Agenten parallel l√§uft, um mit der alten verglichen zu werden). Die Herausforderung besteht darin, zuverl√§ssige Labels oder Bewertungen f√ºr Live-Interaktionen zu erhalten ‚Äì Sie k√∂nnten sich auf Benutzerfeedback oder nachgelagerte Metriken verlassen (z. B. ob der Benutzer auf das Ergebnis geklickt hat).

### Kombination der beiden

Online- und Offline-Bewertungen schlie√üen sich nicht gegenseitig aus; sie erg√§nzen sich stark. Erkenntnisse aus der Online-√úberwachung (z. B. neue Arten von Benutzeranfragen, bei denen der Agent schlecht abschneidet) k√∂nnen verwendet werden, um Offline-Testdatens√§tze zu erweitern und zu verbessern. Umgekehrt k√∂nnen Agenten, die in Offline-Tests gut abschneiden, dann sicherer bereitgestellt und online √ºberwacht werden.

Tats√§chlich √ºbernehmen viele Teams eine Schleife:

_offline bewerten -> bereitstellen -> online √ºberwachen -> neue Fehlerf√§lle sammeln -> zum Offline-Datensatz hinzuf√ºgen -> Agenten verfeinern -> wiederholen_.

## H√§ufige Probleme

Wenn Sie KI-Agenten in der Produktion bereitstellen, k√∂nnen verschiedene Herausforderungen auftreten. Hier sind einige h√§ufige Probleme und m√∂gliche L√∂sungen:

| **Problem**    | **M√∂gliche L√∂sung**   |
| ------------- | ------------------ |
| KI-Agent f√ºhrt Aufgaben nicht konsistent aus | - Verfeinern Sie den Prompt, der dem KI-Agenten gegeben wird; seien Sie klar in den Zielen.<br>- Identifizieren Sie, wo das Aufteilen der Aufgaben in Unteraufgaben und deren Bearbeitung durch mehrere Agenten helfen kann. |
| KI-Agent ger√§t in kontinuierliche Schleifen  | - Stellen Sie sicher, dass Sie klare Beendigungsbedingungen haben, damit der Agent wei√ü, wann der Prozess beendet werden soll.<br>- F√ºr komplexe Aufgaben, die Argumentation und Planung erfordern, verwenden Sie ein gr√∂√üeres Modell, das auf Argumentationsaufgaben spezialisiert ist. |
| Werkzeugaufrufe des KI-Agenten funktionieren nicht gut   | - Testen und validieren Sie die Ausgabe des Werkzeugs au√üerhalb des Agentensystems. |

- Verfeinern Sie die definierten Parameter, Eingabeaufforderungen und die Benennung der Tools.  |
| Multi-Agenten-System funktioniert nicht konsistent | - Verfeinern Sie die Eingabeaufforderungen f√ºr jeden Agenten, um sicherzustellen, dass sie spezifisch und voneinander unterscheidbar sind.<br>- Erstellen Sie ein hierarchisches System mit einem "Routing"- oder Steuerungsagenten, der bestimmt, welcher Agent der richtige ist. |

Viele dieser Probleme k√∂nnen effektiver erkannt werden, wenn eine Beobachtbarkeit implementiert ist. Die zuvor besprochenen Traces und Metriken helfen dabei, genau zu bestimmen, wo im Workflow des Agenten Probleme auftreten, was das Debugging und die Optimierung erheblich effizienter macht.

## Kostenmanagement

Hier sind einige Strategien, um die Kosten f√ºr den Einsatz von KI-Agenten in der Produktion zu verwalten:

**Verwendung kleinerer Modelle:** Kleine Sprachmodelle (Small Language Models, SLMs) k√∂nnen bei bestimmten agentischen Anwendungsf√§llen gut abschneiden und die Kosten erheblich senken. Wie bereits erw√§hnt, ist der Aufbau eines Bewertungssystems, um die Leistung im Vergleich zu gr√∂√üeren Modellen zu bestimmen und zu vergleichen, der beste Weg, um zu verstehen, wie gut ein SLM f√ºr Ihren Anwendungsfall geeignet ist. Ziehen Sie in Betracht, SLMs f√ºr einfachere Aufgaben wie Intent-Klassifikation oder Parameterextraktion zu verwenden, w√§hrend gr√∂√üere Modelle f√ºr komplexes Denken reserviert bleiben.

**Verwendung eines Router-Modells:** Eine √§hnliche Strategie besteht darin, eine Vielfalt von Modellen und Gr√∂√üen zu nutzen. Sie k√∂nnen ein LLM/SLM oder eine serverlose Funktion verwenden, um Anfragen basierend auf ihrer Komplexit√§t an die am besten geeigneten Modelle weiterzuleiten. Dies hilft nicht nur, die Kosten zu senken, sondern stellt auch sicher, dass die Leistung f√ºr die richtigen Aufgaben gew√§hrleistet ist. Beispielsweise k√∂nnen einfache Anfragen an kleinere, schnellere Modelle weitergeleitet werden, w√§hrend teure gro√üe Modelle nur f√ºr komplexe Denkaufgaben verwendet werden.

**Caching von Antworten:** Das Identifizieren h√§ufiger Anfragen und Aufgaben und das Bereitstellen der Antworten, bevor sie durch Ihr agentisches System laufen, ist eine gute M√∂glichkeit, das Volumen √§hnlicher Anfragen zu reduzieren. Sie k√∂nnen sogar einen Ablauf implementieren, um zu erkennen, wie √§hnlich eine Anfrage den gecachten Anfragen ist, indem Sie einfachere KI-Modelle verwenden. Diese Strategie kann die Kosten f√ºr h√§ufig gestellte Fragen oder g√§ngige Workflows erheblich senken.

## Schauen wir uns an, wie das in der Praxis funktioniert

Im [Beispiel-Notebook dieses Abschnitts](../../../10-ai-agents-production/code_samples/10_autogen_evaluation.ipynb) sehen wir Beispiele daf√ºr, wie wir Beobachtungstools nutzen k√∂nnen, um unseren Agenten zu √ºberwachen und zu bewerten.

## Vorherige Lektion

[Metakognitions-Designmuster](../09-metacognition/README.md)

## N√§chste Lektion

[MCP](../11-mcp/README.md)

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser √úbersetzung ergeben.