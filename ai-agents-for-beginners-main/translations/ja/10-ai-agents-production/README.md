<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8164484c16b1ed3287ef9dae9fc437c1",
  "translation_date": "2025-07-23T08:23:20+00:00",
  "source_file": "10-ai-agents-production/README.md",
  "language_code": "ja"
}
-->
# AIエージェントの運用：観測性と評価

AIエージェントが実験的なプロトタイプから実際のアプリケーションへと移行するにつれ、その挙動を理解し、パフォーマンスを監視し、出力を体系的に評価する能力が重要になります。

## 学習目標

このレッスンを完了すると、以下について理解し、実行できるようになります：
- エージェントの観測性と評価の基本概念
- エージェントのパフォーマンス、コスト、効果を向上させる技術
- AIエージェントを体系的に評価する方法と評価すべき内容
- AIエージェントを運用環境に展開する際のコスト管理方法
- AutoGenを使用して構築されたエージェントの計測方法

このレッスンの目的は、「ブラックボックス」エージェントを透明性があり、管理可能で信頼性の高いシステムに変えるための知識を提供することです。

_**注:** 安全で信頼できるAIエージェントを展開することが重要です。[信頼できるAIエージェントの構築](./06-building-trustworthy-agents/README.md)のレッスンもぜひご覧ください。_

## トレースとスパン

[Langfuse](https://langfuse.com/)や[Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-foundry/what-is-azure-ai-foundry)などの観測性ツールは、通常、エージェントの実行をトレースとスパンとして表現します。

- **トレース**は、エージェントのタスク全体を開始から終了まで表します（例：ユーザーのクエリを処理する）。
- **スパン**は、トレース内の個々のステップを表します（例：言語モデルの呼び出しやデータの取得）。

![Langfuseのトレースツリー](https://langfuse.com/images/cookbook/example-autogen-evaluation/trace-tree.png)

観測性がない場合、AIエージェントは「ブラックボックス」のように感じられます。内部状態や推論が不透明で、問題の診断やパフォーマンスの最適化が困難になります。一方、観測性がある場合、エージェントは「ガラスボックス」となり、透明性を提供します。これは信頼を構築し、エージェントが意図通りに動作することを保証するために不可欠です。

## 運用環境で観測性が重要な理由

AIエージェントを運用環境に移行する際には、新たな課題や要件が生じます。観測性はもはや「あると便利」なものではなく、重要な能力となります：

* **デバッグと根本原因の分析**: エージェントが失敗したり予期しない出力を生成した場合、観測性ツールはエラーの原因を特定するために必要なトレースを提供します。特に、複数のLLM呼び出し、ツールの相互作用、条件付きロジックを含む複雑なエージェントでは重要です。
* **遅延とコスト管理**: AIエージェントは、トークンごとや呼び出しごとに課金されるLLMや外部APIに依存することが多いです。観測性によりこれらの呼び出しを正確に追跡し、過度に遅いまたは高価な操作を特定できます。これにより、プロンプトの最適化、より効率的なモデルの選択、ワークフローの再設計が可能となり、運用コストを管理し、良好なユーザー体験を確保できます。
* **信頼性、安全性、コンプライアンス**: 多くのアプリケーションでは、エージェントが安全かつ倫理的に動作することを保証する必要があります。観測性はエージェントの行動や決定の監査証跡を提供します。これにより、プロンプトインジェクション、有害なコンテンツの生成、個人識別情報（PII）の誤処理などの問題を検出し、軽減することができます。例えば、トレースをレビューして、エージェントが特定の応答を提供した理由や特定のツールを使用した理由を理解できます。
* **継続的な改善ループ**: 観測性データは反復的な開発プロセスの基盤です。エージェントが実世界でどのように動作しているかを監視することで、改善点を特定し、モデルの微調整のためのデータを収集し、変更の影響を検証できます。これにより、オンライン評価から得られる運用インサイトがオフラインの実験と改良を情報提供し、エージェントのパフォーマンスを段階的に向上させるフィードバックループが形成されます。

## 追跡すべき主要な指標

エージェントの挙動を監視し理解するためには、さまざまな指標やシグナルを追跡する必要があります。具体的な指標はエージェントの目的によって異なる場合がありますが、いくつかは普遍的に重要です。

以下は、観測性ツールが監視する最も一般的な指標です：

**遅延:** エージェントはどれくらい迅速に応答するか？長い待ち時間はユーザー体験に悪影響を与えます。エージェントの実行をトレースすることで、タスクや個々のステップの遅延を測定できます。例えば、すべてのモデル呼び出しに20秒かかるエージェントは、より高速なモデルを使用するか、モデル呼び出しを並列で実行することで加速できます。

**コスト:** エージェントの実行あたりの費用はどれくらいか？AIエージェントは、トークンごとに課金されるLLM呼び出しや外部APIに依存します。頻繁なツール使用や複数のプロンプトはコストを急速に増加させる可能性があります。例えば、エージェントが品質をわずかに向上させるためにLLMを5回呼び出す場合、そのコストが正当化されるか、呼び出し回数を減らすか、より安価なモデルを使用するべきかを評価する必要があります。リアルタイムの監視は、予期しないスパイク（例：バグによる過剰なAPIループ）を特定するのにも役立ちます。

**リクエストエラー:** エージェントが失敗したリクエストの数はどれくらいか？これにはAPIエラーやツール呼び出しの失敗が含まれます。これらに対してエージェントを運用環境でより堅牢にするために、フォールバックやリトライを設定できます。例えば、LLMプロバイダーAがダウンしている場合、バックアップとしてLLMプロバイダーBに切り替えることができます。

**ユーザーフィードバック:** 直接的なユーザー評価を実装することで貴重な洞察を得られます。これには明示的な評価（👍良い/👎悪い、⭐1-5の星評価）やテキストコメントが含まれます。一貫した否定的なフィードバックは、エージェントが期待通りに動作していない兆候として警告を発するべきです。

**暗黙的なユーザーフィードバック:** ユーザーの行動は、明示的な評価がなくても間接的なフィードバックを提供します。これには、質問の即時再構成、繰り返しのクエリ、またはリトライボタンのクリックが含まれます。例えば、ユーザーが同じ質問を繰り返し尋ねる場合、これはエージェントが期待通りに動作していない兆候です。

**正確性:** エージェントが正しいまたは望ましい出力を生成する頻度はどれくらいか？正確性の定義は異なります（例：問題解決の正確性、情報検索の正確性、ユーザー満足度）。最初のステップは、エージェントにとって成功が何を意味するかを定義することです。自動チェック、評価スコア、タスク完了ラベルを使用して正確性を追跡できます。例えば、トレースを「成功」または「失敗」としてマークすることができます。

**自動評価指標:** 自動評価を設定することも可能です。例えば、エージェントの出力が役立つか正確かどうかをスコアリングするためにLLMを使用することができます。また、エージェントのさまざまな側面をスコアリングするためのオープンソースライブラリもいくつかあります。例：[RAGAS](https://docs.ragas.io/)はRAGエージェント向け、[LLM Guard](https://llm-guard.com/)は有害な言語やプロンプトインジェクションを検出するためのものです。

実際には、これらの指標を組み合わせることで、AIエージェントの健康状態を最も包括的に把握できます。この章の[例のノートブック](../../../10-ai-agents-production/code_samples/10_autogen_evaluation.ipynb)では、これらの指標が実際の例でどのように見えるかを示しますが、まずは典型的な評価ワークフローがどのように見えるかを学びます。

## エージェントの計測

トレースデータを収集するには、コードを計測する必要があります。目標は、エージェントコードを計測して、観測性プラットフォームによってキャプチャ、処理、視覚化できるトレースと指標を生成することです。

**OpenTelemetry (OTel):** [OpenTelemetry](https://opentelemetry.io/)はLLM観測性の業界標準として台頭しています。API、SDK、ツールのセットを提供し、テレメトリデータを生成、収集、エクスポートします。

既存のエージェントフレームワークをラップし、OpenTelemetryスパンを観測性ツールに簡単にエクスポートできる計測ライブラリが多数あります。以下は、[OpenLit計測ライブラリ](https://github.com/openlit/openlit)を使用してAutoGenエージェントを計測する例です：

```python
import openlit

openlit.init(tracer = langfuse._otel_tracer, disable_batch = True)
```

この章の[例のノートブック](../../../10-ai-agents-production/code_samples/10_autogen_evaluation.ipynb)では、AutoGenエージェントを計測する方法を示します。

**手動スパン作成:** 計測ライブラリは良いベースラインを提供しますが、より詳細な情報やカスタム情報が必要な場合があります。手動でスパンを作成してカスタムアプリケーションロジックを追加することができます。さらに、自動的または手動で作成されたスパンにカスタム属性（タグやメタデータとも呼ばれる）を追加して豊かにすることができます。これらの属性には、ビジネス固有のデータ、中間計算、デバッグや分析に役立つコンテキスト（例：`user_id`、`session_id`、`model_version`）が含まれます。

[Langfuse Python SDK](https://langfuse.com/docs/sdk/python/sdk-v3)を使用してトレースとスパンを手動で作成する例：

```python
from langfuse import get_client
 
langfuse = get_client()
 
span = langfuse.start_span(name="my-span")
 
span.end()
```

## エージェント評価

観測性は指標を提供しますが、評価はそのデータを分析し（およびテストを実施し）、AIエージェントがどれだけうまく機能しているか、どのように改善できるかを判断するプロセスです。つまり、トレースや指標を取得した後、それらを使用してエージェントを評価し、意思決定を行う方法です。

定期的な評価は重要です。AIエージェントは非決定論的であり、更新やモデルの挙動の変化を通じて進化する可能性があるため、評価なしでは「スマートエージェント」が実際にうまく機能しているか、または退化しているかを知ることができません。

AIエージェントの評価には、**オンライン評価**と**オフライン評価**の2つのカテゴリがあります。どちらも価値があり、互いに補完し合います。通常、オフライン評価から始めます。これはエージェントを展開する前に最低限必要なステップです。

### オフライン評価

![Langfuseのデータセット項目](https://langfuse.com/images/cookbook/example-autogen-evaluation/example-dataset.png)

これは、制御された環境でエージェントを評価することを指します。通常、テストデータセットを使用し、ライブユーザーのクエリは使用しません。期待される出力や正しい挙動が分かっているデータセットを使用し、エージェントを実行します。

例えば、数学の文章問題エージェントを構築した場合、既知の答えを持つ[テストデータセット](https://huggingface.co/datasets/gsm8k)を100問用意することができます。オフライン評価は開発中に行われることが多く（CI/CDパイプラインの一部になることもあります）、改善を確認したり退化を防ぐために使用されます。その利点は、**再現可能であり、正解があるため明確な正確性指標を得られること**です。ユーザーのクエリをシミュレーションし、エージェントの応答を理想的な答えと比較したり、前述の自動指標を使用して測定することもできます。

オフライン評価の主な課題は、テストデータセットが包括的で関連性を維持することです。エージェントが固定されたテストセットで良好なパフォーマンスを示しても、運用環境では非常に異なるクエリに直面する可能性があります。そのため、テストセットを新しいエッジケースや実世界のシナリオを反映した例で更新し続ける必要があります。小規模な「スモークテスト」ケースと大規模な評価セットを組み合わせることが有用です：小規模なセットは迅速なチェック用、大規模なセットは広範なパフォーマンス指標用です。

### オンライン評価

![観測性指標の概要](https://langfuse.com/images/cookbook/example-autogen-evaluation/dashboard.png)

これは、ライブの実世界環境、つまり実際の運用中にエージェントを評価することを指します。オンライン評価では、エージェントのパフォーマンスを実際のユーザーとのインタラクションで継続的に監視し、結果を分析します。

例えば、成功率、ユーザー満足度スコア、またはライブトラフィックの他の指標を追跡することができます。オンライン評価の利点は、**ラボ環境では予期しないことを捉えることができる**点です。モデルのドリフト（入力パターンの変化に伴いエージェントの有効性が低下すること）を観察したり、テストデータには含まれていない予期しないクエリや状況を捉えることができます。これにより、エージェントが実際の環境でどのように動作しているかの真の姿を把握できます。

オンライン評価では、前述のように暗黙的および明示的なユーザーフィードバックを収集したり、シャドウテストやA/Bテスト（新しいバージョンのエージェントを旧バージョンと並行して実行して比較する）を実施することがあります。課題としては、ライブインタラクションの信頼性の高いラベルやスコアを取得するのが難しい場合があります。ユーザーフィードバックや下流の指標（例：ユーザーが結果をクリックしたかどうか）に依存することがあります。

### 両者の組み合わせ

オンライン評価とオフライン評価は相互排他的ではなく、非常に補完的です。オンライン監視から得られるインサイト（例：エージェントがうまく機能していない新しいタイプのユーザークエリ）は、オフラインのテストデータセットを拡張し改善するために使用できます。逆に、オフラインテストで良好なパフォーマンスを示

- 定義されたパラメータ、プロンプト、ツールの名前を洗練させる。  
| マルチエージェントシステムが一貫して動作しない | - 各エージェントに与えるプロンプトを見直し、それぞれが具体的で明確に区別されるようにする。<br>- 「ルーティング」またはコントローラーエージェントを使用して、どのエージェントが適切かを判断する階層的なシステムを構築する。 |

これらの問題の多くは、可観測性を導入することでより効果的に特定できます。前述したトレースやメトリクスは、エージェントのワークフローのどこで問題が発生しているかを正確に特定するのに役立ち、デバッグや最適化を大幅に効率化します。

## コスト管理

AIエージェントを本番環境にデプロイする際のコストを管理するための戦略を以下に示します：

**小型モデルの使用:** Small Language Models (SLMs) は、特定のエージェントユースケースで十分な性能を発揮し、コストを大幅に削減できます。前述のように、評価システムを構築して大規模モデルとの性能を比較することで、SLMがユースケースにどの程度適しているかを理解するのが最善です。意図の分類やパラメータ抽出のような単純なタスクにはSLMを使用し、複雑な推論には大規模モデルを使用することを検討してください。

**ルーターモデルの使用:** 似たような戦略として、さまざまなモデルやサイズを組み合わせて使用する方法があります。LLM/SLMやサーバーレス関数を使用して、リクエストの複雑さに応じて最適なモデルにルーティングすることができます。これにより、コストを削減しつつ、適切なタスクでの性能を確保できます。例えば、単純なクエリは小型で高速なモデルにルーティングし、複雑な推論タスクには高価な大規模モデルを使用する、といった方法です。

**レスポンスのキャッシュ:** 共通のリクエストやタスクを特定し、それらのレスポンスをエージェントシステムを通す前に提供することで、類似リクエストの量を削減できます。さらに、より基本的なAIモデルを使用して、リクエストがキャッシュされたリクエストとどの程度類似しているかを特定するフローを実装することも可能です。この戦略は、よくある質問や一般的なワークフローに対してコストを大幅に削減するのに役立ちます。

## 実際にどのように機能するか見てみましょう

このセクションの[サンプルノートブック](../../../10-ai-agents-production/code_samples/10_autogen_evaluation.ipynb)では、可観測性ツールを使用してエージェントを監視・評価する方法の例を確認できます。

## 前のレッスン

[メタ認知デザインパターン](../09-metacognition/README.md)

## 次のレッスン

[MCP](../11-mcp/README.md)

**免責事項**:  
この文書は、AI翻訳サービス [Co-op Translator](https://github.com/Azure/co-op-translator) を使用して翻訳されています。正確性を追求しておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があることをご承知ください。元の言語で記載された文書が正式な情報源とみなされるべきです。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用に起因する誤解や誤解釈について、当方は一切の責任を負いません。