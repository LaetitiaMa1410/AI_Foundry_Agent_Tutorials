{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch evaluation with your custom evaluators\n",
    "The following sample shows the basic way to evaluate a Generative AI application in your development environment with your custom evaluators.\n",
    "\n",
    "> ‚ú® ***Note*** <br>\n",
    "> Please check the reference document before you get started - https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/evaluate-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "## üî® Current Support and Limitations (as of 2025-01-14) \n",
    "- Check the region support for the Azure AI Evaluation SDK. https://learn.microsoft.com/en-us/azure/ai-studio/concepts/evaluation-metrics-built-in?tabs=warning#region-support\n",
    "\n",
    "### Region support for evaluations\n",
    "| Region              | Hate and Unfairness, Sexual, Violent, Self-Harm, XPIA, ECI (Text) | Groundedness (Text) | Protected Material (Text) | Hate and Unfairness, Sexual, Violent, Self-Harm, Protected Material (Image) |\n",
    "|---------------------|------------------------------------------------------------------|---------------------|----------------------------|----------------------------------------------------------------------------|\n",
    "| North Central US    | no                                                               | no                  | no                         | yes                                                                        |\n",
    "| East US 2           | yes                                                              | yes                 | yes                        | yes                                                                        |\n",
    "| Sweden Central      | yes                                                              | yes                 | yes                        | yes                                                                        |\n",
    "| US North Central    | yes                                                              | no                  | yes                        | yes                                                                        |\n",
    "| France Central      | yes                                                              | yes                 | yes                        | yes                                                                        |\n",
    "| Switzerland West    | yes                                                              | no                  | no                         | yes                                                                        |\n",
    "\n",
    "### Region support for adversarial simulation\n",
    "| Region            | Adversarial Simulation (Text) | Adversarial Simulation (Image) |\n",
    "|-------------------|-------------------------------|---------------------------------|\n",
    "| UK South          | yes                           | no                              |\n",
    "| East US 2         | yes                           | yes                             |\n",
    "| Sweden Central    | yes                           | yes                             |\n",
    "| US North Central  | yes                           | yes                             |\n",
    "| France Central    | yes                           | no                              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úîÔ∏è Pricing and billing\n",
    "- Effective 1/14/2025, Azure AI Safety Evaluations will no longer be free in public preview. It will be billed based on consumption as following:\n",
    "\n",
    "| Service Name              | Safety Evaluations       | Price Per 1K Tokens (USD) |\n",
    "|---------------------------|--------------------------|---------------------------|\n",
    "| Azure Machine Learning    | Input pricing for 3P     | $0.02                     |\n",
    "| Azure Machine Learning    | Output pricing for 3P    | $0.06                     |\n",
    "| Azure Machine Learning    | Input pricing for 1P     | $0.012                    |\n",
    "| Azure Machine Learning    | Output pricing for 1P    | $0.012                    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "from pprint import pprint\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import RelevanceEvaluator\n",
    "from azure.ai.evaluation import GroundednessEvaluator, GroundednessProEvaluator\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import (\n",
    "    Evaluation,\n",
    "    EvaluatorConfiguration,\n",
    ")\n",
    "import pathlib\n",
    "\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import (\n",
    "    ContentSafetyEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    CoherenceEvaluator,\n",
    "    GroundednessEvaluator,\n",
    "    FluencyEvaluator,\n",
    "    SimilarityEvaluator,\n",
    "    F1ScoreEvaluator,\n",
    "    RetrievalEvaluator,\n",
    ")\n",
    "\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Run Evaluators in Azure Cloud (azure.ai.evaluation.evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "credential = DefaultAzureCredential()\n",
    "\n",
    "azure_ai_project_endpoint = os.environ.get(\"AZURE_AI_PROJECT_ENDPOINT\")\n",
    "project_resource_id = os.environ.get(\"AZURE_AI_PROJECT_RESOURCE_ID\")\n",
    "subscription_id = project_resource_id.split(\"/\")[2]\n",
    "resource_group_name = project_resource_id.split(\"/\")[4]\n",
    "project_name = azure_ai_project_endpoint.split(\"/\")[5]\n",
    "\n",
    "\n",
    "azure_ai_project_dict = {\n",
    "    \"subscription_id\": subscription_id,\n",
    "    \"resource_group_name\": resource_group_name,\n",
    "    \"project_name\": project_name,\n",
    "}\n",
    "\n",
    "azure_ai_project_client = AIProjectClient(\n",
    "    credential=DefaultAzureCredential(), \n",
    "    endpoint=azure_ai_project_endpoint,\n",
    ")\n",
    "\n",
    "\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"),\n",
    "    \"api_version\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    \"type\": \"azure_openai\",\n",
    "}\n",
    "\n",
    "ml_client = MLClient(credential, subscription_id, resource_group_name, project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set the evaluation dataset\n",
    "- Use your query data set on your storage account. These response records serve as the seed for creating assessments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Initialized AzuureOpenAI client ===\n",
      "AZURE_OPENAI_ENDPOINT=https://aoai-services1.openai.azure.com/\n",
      "AZURE_OPENAI_API_VERSION=2025-03-01-preview\n",
      "AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "\n",
    "aoai_api_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "aoai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "aoai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "aoai_deployment_name = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "\n",
    "try:\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=aoai_api_endpoint,\n",
    "        api_key=aoai_api_key,\n",
    "        api_version=aoai_api_version,\n",
    "    )\n",
    "\n",
    "    print(\"=== Initialized AzuureOpenAI client ===\")\n",
    "    print(f\"AZURE_OPENAI_ENDPOINT={aoai_api_endpoint}\")\n",
    "    print(f\"AZURE_OPENAI_API_VERSION={aoai_api_version}\")\n",
    "    print(f\"AZURE_OPENAI_DEPLOYMENT_NAME={aoai_deployment_name}\")\n",
    "\n",
    "except (ValueError, TypeError) as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You have all the data (Query + Context + Response + Ground Truth)\n",
    "- Assume that you already have all the data (Query + Context + Response + Ground Truth) in a local folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"./data/sythetic_evaluation_data.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Run Evaluators for local and upload to cloud (azure.ai.evaluation.evaluate)\n",
    "- set up your custom GroundnessEvaluator, ExactMatchEvaluator for Local environment with the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing_extensions import override\n",
    "\n",
    "class CustomGroundednessEvaluator(GroundednessEvaluator):\n",
    "    \"\"\"\n",
    "    Evaluates groundedness score for a given query (optional), response, and context or a multi-turn conversation,\n",
    "    including reasoning.\n",
    "\n",
    "    The groundedness measure assesses the correspondence between claims in an AI-generated answer and the source\n",
    "    context, making sure that these claims are substantiated by the context. Even if the responses from LLM are\n",
    "    factually correct, they'll be considered ungrounded if they can't be verified against the provided sources\n",
    "    (such as your input source or your database). Use the groundedness metric when you need to verify that\n",
    "    AI-generated responses align with and are validated by the provided context.\n",
    "\n",
    "    Groundedness scores range from 0.0 to 1.0, with 0.0 being the least grounded and 1.0 being the grounded.\n",
    "\n",
    "    :param model_config: Configuration for the Azure OpenAI model.\n",
    "    :type model_config: Union[~azure.ai.evaluation.AzureOpenAIModelConfiguration,\n",
    "        ~azure.ai.evaluation.OpenAIModelConfiguration]\n",
    "\n",
    "    .. admonition:: Example:\n",
    "\n",
    "        .. literalinclude:: ../samples/evaluation_samples_evaluate.py\n",
    "            :start-after: [START groundedness_evaluator]\n",
    "            :end-before: [END groundedness_evaluator]\n",
    "            :language: python\n",
    "            :dedent: 8\n",
    "            :caption: Initialize and call a GroundednessEvaluator.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        To align with our support of a diverse set of models, an output key without the `gpt_` prefix has been added.\n",
    "        To maintain backwards compatibility, the old key with the `gpt_` prefix is still be present in the output;\n",
    "        however, it is recommended to use the new key moving forward as the old key will be deprecated in the future.\n",
    "    \"\"\"\n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "    # need to set the new prompty file path because the variables are still used in the parent call method\n",
    "    _PROMPTY_FILE_NO_QUERY = os.path.join(current_dir, 'custom-groundedness.prompty')\n",
    "    _PROMPTY_FILE_WITH_QUERY = os.path.join(current_dir, 'custom-groundedness.prompty')\n",
    "\n",
    "    \n",
    "    @override\n",
    "    def __init__(self, model_config):\n",
    "        \n",
    "        super().__init__(model_config)\n",
    "        current_dir = os.getcwd()\n",
    "        prompty_path = os.path.join(current_dir, \"custom-groundedness.prompty\")  # Default to no query\n",
    "        super(GroundednessEvaluator, self).__init__(model_config=model_config, prompty_file=prompty_path, result_key=\"custom-groundedness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    class ExactMatchEvaluator(EvaluatorBase):\\n    \"\"\"\\n    Evaluates whether the response exactly matches the ground truth.\\n\\n    This evaluator returns a score of 1.0 if the response is identical to the ground truth,\\n    and 0.0 otherwise. It is useful for tasks that require strict correctness such as factual QA.\\n\\n    Example:\\n        evaluator = ExactMatchEvaluator()\\n        result = evaluator(ground_truth=\"Hello, world!\", response=\"Hello, world!\")\\n        print(result)  # {\\'exact_match_score\\': 1.0}\\n    \"\"\"\\n\\n    id = \"update with your azure ml asset id\"\\n    \"\"\"Evaluator identifier, experimental and to be used only with evaluation in cloud.\"\"\"\\n\\n    @override\\n    def __init__(self):\\n        super().__init__()\\n\\n    @override\\n    async def _do_eval(self, eval_input: Dict) -> Dict[str, float]:\\n        \"\"\"Evaluate whether the response matches the ground truth exactly.\"\"\"\\n        ground_truth = eval_input[\"ground_truth\"].strip()\\n        response = eval_input[\"response\"].strip()\\n\\n        score = 1.0 if ground_truth == response else 0.0\\n\\n        return {\\n            \"exact_match_score\": score,\\n        }\\n\\n    @overload\\n    def __call__(self, *, ground_truth: str, response: str):\\n        \"\"\"\\n        Evaluate whether the response matches the ground truth exactly.\\n\\n        :keyword response: The response to be evaluated.\\n        :paramtype response: str\\n        :keyword ground_truth: The ground truth to be compared against.\\n        :paramtype ground_truth: str\\n        :return: The exact match score.\\n        :rtype: Dict[str, float]\\n        \"\"\"\\n\\n    @override\\n    def __call__(self, *args, **kwargs):\\n        \"\"\"Evaluate whether the response matches the ground truth exactly.\"\"\"\\n        return super().__call__(*args, **kwargs)\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ExactMatchEvaluator._exact_match import ExactMatchEvaluator\n",
    "\n",
    "# check the source code of the ExactMatchEvaluator\n",
    "# This is a custom evaluator that relates to the task of evaluating in local environment\n",
    "'''\n",
    "    class ExactMatchEvaluator(EvaluatorBase):\n",
    "    \"\"\"\n",
    "    Evaluates whether the response exactly matches the ground truth.\n",
    "\n",
    "    This evaluator returns a score of 1.0 if the response is identical to the ground truth,\n",
    "    and 0.0 otherwise. It is useful for tasks that require strict correctness such as factual QA.\n",
    "\n",
    "    Example:\n",
    "        evaluator = ExactMatchEvaluator()\n",
    "        result = evaluator(ground_truth=\"Hello, world!\", response=\"Hello, world!\")\n",
    "        print(result)  # {'exact_match_score': 1.0}\n",
    "    \"\"\"\n",
    "\n",
    "    id = \"update with your azure ml asset id\"\n",
    "    \"\"\"Evaluator identifier, experimental and to be used only with evaluation in cloud.\"\"\"\n",
    "\n",
    "    @override\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @override\n",
    "    async def _do_eval(self, eval_input: Dict) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate whether the response matches the ground truth exactly.\"\"\"\n",
    "        ground_truth = eval_input[\"ground_truth\"].strip()\n",
    "        response = eval_input[\"response\"].strip()\n",
    "        \n",
    "        score = 1.0 if ground_truth == response else 0.0\n",
    "\n",
    "        return {\n",
    "            \"exact_match_score\": score,\n",
    "        }\n",
    "\n",
    "    @overload\n",
    "    def __call__(self, *, ground_truth: str, response: str):\n",
    "        \"\"\"\n",
    "        Evaluate whether the response matches the ground truth exactly.\n",
    "\n",
    "        :keyword response: The response to be evaluated.\n",
    "        :paramtype response: str\n",
    "        :keyword ground_truth: The ground truth to be compared against.\n",
    "        :paramtype ground_truth: str\n",
    "        :return: The exact match score.\n",
    "        :rtype: Dict[str, float]\n",
    "        \"\"\"\n",
    "\n",
    "    @override\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"Evaluate whether the response matches the ground truth exactly.\"\"\"\n",
    "        return super().__call__(*args, **kwargs)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match_score': 1.0,\n",
       " 'exact_match_threshold': 3.0,\n",
       " 'exact_match_result': 'fail'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exactMatchEvaluator = ExactMatchEvaluator()\n",
    "\n",
    "exactMatch = exactMatchEvaluator(\n",
    "    ground_truth=\"What is the speed of light?\", response=\"What is the speed of light?\"\n",
    ")\n",
    "\n",
    "exactMatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = input_path  \n",
    "output_path = \"./data/local_upload_cloud_evaluation_output.json\"\n",
    "\n",
    "\n",
    "# https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/flow-evaluate-sdk\n",
    "\n",
    "retrieval_evaluator = RetrievalEvaluator(model_config)\n",
    "custom_groundedness_evaluator = CustomGroundednessEvaluator(model_config)\n",
    "relevance_evaluator = RelevanceEvaluator(model_config)\n",
    "similarity_evaluator = SimilarityEvaluator(model_config)\n",
    "exactMatchEvaluator = ExactMatchEvaluator()\n",
    "\n",
    "column_mapping = {\n",
    "    \"query\": \"${data.query}\",\n",
    "    \"ground_truth\": \"${data.ground_truth}\",\n",
    "    \"response\": \"${data.response}\",\n",
    "    \"context\": \"${data.context}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-20 10:00:07 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-06-20 10:00:07 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-06-20 10:00:07 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_retrieval_20250620_100007_562606, log path: /home/azureuser/.promptflow/.runs/azure_ai_evaluation_evaluators_retrieval_20250620_100007_562606/logs.txt\n",
      "[2025-06-20 10:00:07 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_relevance_20250620_100007_563517, log path: /home/azureuser/.promptflow/.runs/azure_ai_evaluation_evaluators_relevance_20250620_100007_563517/logs.txt\n",
      "[2025-06-20 10:00:07 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-06-20 10:00:07 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-06-20 10:00:07 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-06-20 10:00:07 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_similarity_20250620_100007_565304, log path: /home/azureuser/.promptflow/.runs/azure_ai_evaluation_evaluators_similarity_20250620_100007_565304/logs.txt\n",
      "[2025-06-20 10:00:07 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_custom-groundedness_20250620_100007_563079, log path: /home/azureuser/.promptflow/.runs/azure_ai_evaluation_evaluators_custom-groundedness_20250620_100007_563079/logs.txt\n",
      "[2025-06-20 10:00:07 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_exact_match_20250620_100007_569323, log path: /home/azureuser/.promptflow/.runs/azure_ai_evaluation_evaluators_exact_match_20250620_100007_569323/logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-20 10:00:09 +0000  536624 execution.bulk     INFO     Finished 11 / 19 lines.\n",
      "2025-06-20 10:00:09 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.12 seconds. Estimated time for incomplete lines: 0.96 seconds.\n",
      "2025-06-20 10:00:09 +0000  536624 execution.bulk     INFO     Finished 12 / 19 lines.\n",
      "2025-06-20 10:00:09 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.11 seconds. Estimated time for incomplete lines: 0.77 seconds.\n",
      "2025-06-20 10:00:09 +0000  536624 execution.bulk     INFO     Finished 13 / 19 lines.\n",
      "2025-06-20 10:00:09 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.11 seconds. Estimated time for incomplete lines: 0.66 seconds.\n",
      "2025-06-20 10:00:09 +0000  536624 execution.bulk     INFO     Finished 14 / 19 lines.\n",
      "2025-06-20 10:00:09 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.1 seconds. Estimated time for incomplete lines: 0.5 seconds.\n",
      "2025-06-20 10:00:09 +0000  536624 execution.bulk     INFO     Finished 15 / 19 lines.\n",
      "2025-06-20 10:00:09 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.09 seconds. Estimated time for incomplete lines: 0.36 seconds.\n",
      "2025-06-20 10:00:09 +0000  536624 execution.bulk     INFO     Finished 16 / 19 lines.\n",
      "2025-06-20 10:00:09 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.09 seconds. Estimated time for incomplete lines: 0.27 seconds.\n",
      "2025-06-20 10:00:09 +0000  536624 execution.bulk     INFO     Finished 17 / 19 lines.\n",
      "2025-06-20 10:00:09 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.08 seconds. Estimated time for incomplete lines: 0.16 seconds.\n",
      "2025-06-20 10:00:09 +0000  536624 execution.bulk     INFO     Finished 18 / 19 lines.\n",
      "2025-06-20 10:00:09 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.08 seconds. Estimated time for incomplete lines: 0.08 seconds.\n",
      "2025-06-20 10:00:09 +0000  536624 execution.bulk     INFO     Finished 19 / 19 lines.\n",
      "2025-06-20 10:00:09 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.08 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-06-20 10:00:10 +0000  536624 execution.bulk     INFO     Finished 1 / 19 lines.\n",
      "2025-06-20 10:00:10 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 2.52 seconds. Estimated time for incomplete lines: 45.36 seconds.\n",
      "2025-06-20 10:00:10 +0000  536624 execution.bulk     INFO     Finished 1 / 19 lines.\n",
      "2025-06-20 10:00:10 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 3.13 seconds. Estimated time for incomplete lines: 56.34 seconds.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Finished 1 / 19 lines.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 3.43 seconds. Estimated time for incomplete lines: 61.74 seconds.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Finished 2 / 19 lines.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 1.68 seconds. Estimated time for incomplete lines: 28.56 seconds.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Finished 2 / 19 lines.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 1.84 seconds. Estimated time for incomplete lines: 31.28 seconds.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Finished 3 / 19 lines.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 1.24 seconds. Estimated time for incomplete lines: 19.84 seconds.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Finished 2 / 19 lines.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 1.85 seconds. Estimated time for incomplete lines: 31.45 seconds.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Finished 4 / 19 lines.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.9 seconds. Estimated time for incomplete lines: 13.5 seconds.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Finished 3 / 19 lines.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 1.24 seconds. Estimated time for incomplete lines: 19.84 seconds.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Finished 4 / 19 lines.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.96 seconds. Estimated time for incomplete lines: 14.4 seconds.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Finished 4 / 19 lines.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.96 seconds. Estimated time for incomplete lines: 14.4 seconds.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Finished 5 / 19 lines.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.78 seconds. Estimated time for incomplete lines: 10.92 seconds.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Finished 5 / 19 lines.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.76 seconds. Estimated time for incomplete lines: 10.64 seconds.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Finished 5 / 19 lines.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.78 seconds. Estimated time for incomplete lines: 10.92 seconds.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Finished 6 / 19 lines.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.66 seconds. Estimated time for incomplete lines: 8.58 seconds.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Finished 7 / 19 lines.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.57 seconds. Estimated time for incomplete lines: 6.84 seconds.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Finished 6 / 19 lines.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.66 seconds. Estimated time for incomplete lines: 8.58 seconds.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Finished 8 / 19 lines.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.51 seconds. Estimated time for incomplete lines: 5.61 seconds.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Finished 7 / 19 lines.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.59 seconds. Estimated time for incomplete lines: 7.08 seconds.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Finished 9 / 19 lines.\n",
      "2025-06-20 10:00:11 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.47 seconds. Estimated time for incomplete lines: 4.7 seconds.\n",
      "2025-06-20 10:00:12 +0000  536624 execution.bulk     INFO     Finished 6 / 19 lines.\n",
      "2025-06-20 10:00:12 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.69 seconds. Estimated time for incomplete lines: 8.97 seconds.\n",
      "2025-06-20 10:00:12 +0000  536624 execution.bulk     INFO     Finished 7 / 19 lines.\n",
      "2025-06-20 10:00:12 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.61 seconds. Estimated time for incomplete lines: 7.32 seconds.\n",
      "2025-06-20 10:00:12 +0000  536624 execution.bulk     INFO     Finished 8 / 19 lines.\n",
      "2025-06-20 10:00:12 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.54 seconds. Estimated time for incomplete lines: 5.94 seconds.\n",
      "2025-06-20 10:00:12 +0000  536624 execution.bulk     INFO     Finished 10 / 19 lines.\n",
      "2025-06-20 10:00:12 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.45 seconds. Estimated time for incomplete lines: 4.05 seconds.\n",
      "2025-06-20 10:00:12 +0000  536624 execution.bulk     INFO     Finished 8 / 19 lines.\n",
      "2025-06-20 10:00:12 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.56 seconds. Estimated time for incomplete lines: 6.16 seconds.\n",
      "2025-06-20 10:00:12 +0000  536624 execution.bulk     INFO     Finished 9 / 19 lines.\n",
      "2025-06-20 10:00:12 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.51 seconds. Estimated time for incomplete lines: 5.1 seconds.\n",
      "2025-06-20 10:00:12 +0000  536624 execution.bulk     INFO     Finished 10 / 19 lines.\n",
      "2025-06-20 10:00:12 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.47 seconds. Estimated time for incomplete lines: 4.23 seconds.\n",
      "2025-06-20 10:00:12 +0000  536624 execution.bulk     INFO     Finished 9 / 19 lines.\n",
      "2025-06-20 10:00:12 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.56 seconds. Estimated time for incomplete lines: 5.6 seconds.\n",
      "2025-06-20 10:00:13 +0000  536624 execution.bulk     INFO     Finished 10 / 19 lines.\n",
      "2025-06-20 10:00:13 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.53 seconds. Estimated time for incomplete lines: 4.77 seconds.\n",
      "2025-06-20 10:00:13 +0000  536624 execution.bulk     INFO     Finished 11 / 19 lines.\n",
      "2025-06-20 10:00:13 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.55 seconds. Estimated time for incomplete lines: 4.4 seconds.\n",
      "2025-06-20 10:00:14 +0000  536624 execution.bulk     INFO     Finished 11 / 19 lines.\n",
      "2025-06-20 10:00:14 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.57 seconds. Estimated time for incomplete lines: 4.56 seconds.\n",
      "2025-06-20 10:00:14 +0000  536624 execution.bulk     INFO     Finished 12 / 19 lines.\n",
      "2025-06-20 10:00:14 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.53 seconds. Estimated time for incomplete lines: 3.71 seconds.\n",
      "2025-06-20 10:00:14 +0000  536624 execution.bulk     INFO     Finished 12 / 19 lines.\n",
      "2025-06-20 10:00:14 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.54 seconds. Estimated time for incomplete lines: 3.78 seconds.\n",
      "2025-06-20 10:00:14 +0000  536624 execution.bulk     INFO     Finished 13 / 19 lines.\n",
      "2025-06-20 10:00:14 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.5 seconds. Estimated time for incomplete lines: 3.0 seconds.\n",
      "2025-06-20 10:00:14 +0000  536624 execution.bulk     INFO     Finished 13 / 19 lines.\n",
      "2025-06-20 10:00:14 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.51 seconds. Estimated time for incomplete lines: 3.06 seconds.\n",
      "2025-06-20 10:00:14 +0000  536624 execution.bulk     INFO     Finished 14 / 19 lines.\n",
      "2025-06-20 10:00:14 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.47 seconds. Estimated time for incomplete lines: 2.35 seconds.\n",
      "2025-06-20 10:00:14 +0000  536624 execution.bulk     INFO     Finished 11 / 19 lines.\n",
      "2025-06-20 10:00:14 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.61 seconds. Estimated time for incomplete lines: 4.88 seconds.\n",
      "2025-06-20 10:00:14 +0000  536624 execution.bulk     INFO     Finished 15 / 19 lines.\n",
      "2025-06-20 10:00:14 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.45 seconds. Estimated time for incomplete lines: 1.8 seconds.\n",
      "2025-06-20 10:00:14 +0000  536624 execution.bulk     INFO     Finished 12 / 19 lines.\n",
      "2025-06-20 10:00:14 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.57 seconds. Estimated time for incomplete lines: 3.99 seconds.\n",
      "2025-06-20 10:00:14 +0000  536624 execution.bulk     INFO     Finished 13 / 19 lines.\n",
      "2025-06-20 10:00:14 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.53 seconds. Estimated time for incomplete lines: 3.18 seconds.\n",
      "2025-06-20 10:00:14 +0000  536624 execution.bulk     INFO     Finished 14 / 19 lines.\n",
      "2025-06-20 10:00:14 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.49 seconds. Estimated time for incomplete lines: 2.45 seconds.\n",
      "2025-06-20 10:00:14 +0000  536624 execution.bulk     INFO     Finished 15 / 19 lines.\n",
      "2025-06-20 10:00:14 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.47 seconds. Estimated time for incomplete lines: 1.88 seconds.\n",
      "2025-06-20 10:00:14 +0000  536624 execution.bulk     INFO     Finished 14 / 19 lines.\n",
      "2025-06-20 10:00:14 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.51 seconds. Estimated time for incomplete lines: 2.55 seconds.\n",
      "2025-06-20 10:00:14 +0000  536624 execution.bulk     INFO     Finished 16 / 19 lines.\n",
      "2025-06-20 10:00:14 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.45 seconds. Estimated time for incomplete lines: 1.35 seconds.\n",
      "2025-06-20 10:00:15 +0000  536624 execution.bulk     INFO     Finished 16 / 19 lines.\n",
      "2025-06-20 10:00:15 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.46 seconds. Estimated time for incomplete lines: 1.38 seconds.\n",
      "2025-06-20 10:00:15 +0000  536624 execution.bulk     INFO     Finished 15 / 19 lines.\n",
      "2025-06-20 10:00:15 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.49 seconds. Estimated time for incomplete lines: 1.96 seconds.\n",
      "2025-06-20 10:00:15 +0000  536624 execution.bulk     INFO     Finished 16 / 19 lines.\n",
      "2025-06-20 10:00:15 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.47 seconds. Estimated time for incomplete lines: 1.41 seconds.\n",
      "2025-06-20 10:00:15 +0000  536624 execution.bulk     INFO     Finished 17 / 19 lines.\n",
      "2025-06-20 10:00:15 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.44 seconds. Estimated time for incomplete lines: 0.88 seconds.\n",
      "2025-06-20 10:00:15 +0000  536624 execution.bulk     INFO     Finished 18 / 19 lines.\n",
      "2025-06-20 10:00:15 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.43 seconds. Estimated time for incomplete lines: 0.43 seconds.\n",
      "2025-06-20 10:00:15 +0000  536624 execution.bulk     INFO     Finished 17 / 19 lines.\n",
      "2025-06-20 10:00:15 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.45 seconds. Estimated time for incomplete lines: 0.9 seconds.\n",
      "2025-06-20 10:00:15 +0000  536624 execution.bulk     INFO     Finished 18 / 19 lines.\n",
      "2025-06-20 10:00:15 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.43 seconds. Estimated time for incomplete lines: 0.43 seconds.\n",
      "2025-06-20 10:00:15 +0000  536624 execution.bulk     INFO     Finished 19 / 19 lines.\n",
      "2025-06-20 10:00:15 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.43 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-06-20 10:00:16 +0000  536624 execution.bulk     INFO     Finished 17 / 19 lines.\n",
      "2025-06-20 10:00:16 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.51 seconds. Estimated time for incomplete lines: 1.02 seconds.\n",
      "2025-06-20 10:00:17 +0000  536624 execution.bulk     INFO     Finished 19 / 19 lines.\n",
      "2025-06-20 10:00:17 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.49 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-06-20 10:00:17 +0000  536624 execution.bulk     INFO     Finished 18 / 19 lines.\n",
      "2025-06-20 10:00:17 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.52 seconds. Estimated time for incomplete lines: 0.52 seconds.\n",
      "2025-06-20 10:00:25 +0000  536624 execution.bulk     INFO     Finished 19 / 19 lines.\n",
      "2025-06-20 10:00:25 +0000  536624 execution.bulk     INFO     Average execution time for completed lines: 0.92 seconds. Estimated time for incomplete lines: 0.0 seconds.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "result = evaluate(\n",
    "    evaluation_name=f\"custom_evaluation_local_upload_cloud_{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "    data=input_path,\n",
    "    evaluators={\n",
    "        \"retrieval\": retrieval_evaluator,\n",
    "        \"custom-groundedness\": custom_groundedness_evaluator,\n",
    "        \"relevance\": relevance_evaluator,\n",
    "        \"similarity\": similarity_evaluator,\n",
    "        \"exact_match\": exactMatchEvaluator,\n",
    "    },\n",
    "    evaluator_config={\n",
    "        \"retrieval\": {\"column_mapping\": column_mapping},\n",
    "        \"custom-groundedness\": {\"column_mapping\": column_mapping},\n",
    "        \"relevance\": {\"column_mapping\": column_mapping},\n",
    "        \"similarity\": {\"column_mapping\": column_mapping},\n",
    "        \"exact_match\": {\"column_mapping\": column_mapping},\n",
    "    },\n",
    "    output_path=output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize evaluation Results as html\n",
    "- You can visualize the evaluation results as HTML with the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "def generate_evaluation_report(data_file):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Load JSON file\n",
    "    with open(data_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    rows = data.get('rows', [])\n",
    "\n",
    "    # Define score ranges\n",
    "    score_range = [0, 1, 2, 3, 4, 5]\n",
    "    freq_retrieval = {score: 0 for score in score_range}\n",
    "    freq_relevance = {score: 0 for score in score_range}\n",
    "    freq_similarity = {score: 0 for score in score_range}\n",
    "    freq_groundedness = {score: 0 for score in score_range}\n",
    "    freq_exact_match = {score: 0 for score in [0, 1]}\n",
    "\n",
    "    # Count occurrences of each score\n",
    "    for row in rows:\n",
    "        retrieval_score = row.get('outputs.retrieval.retrieval', 0)\n",
    "        relevance_score = row.get('outputs.relevance.relevance', 0)\n",
    "        similarity_score = row.get('outputs.similarity.similarity', 0)\n",
    "        groundedness_score = row.get('outputs.custom-groundedness.custom-groundedness', 0)\n",
    "        exact_match_score = row.get('outputs.exact_match.exact_match_score', 0)\n",
    "        \n",
    "        if retrieval_score in freq_retrieval:\n",
    "            freq_retrieval[retrieval_score] += 1\n",
    "        if relevance_score in freq_relevance:\n",
    "            freq_relevance[relevance_score] += 1\n",
    "        if similarity_score in freq_similarity:\n",
    "            freq_similarity[similarity_score] += 1\n",
    "        if groundedness_score in freq_groundedness:\n",
    "            freq_groundedness[groundedness_score] += 1\n",
    "        if exact_match_score in freq_exact_match:\n",
    "            freq_exact_match[exact_match_score] += 1\n",
    "\n",
    "    # Function to generate bar chart\n",
    "    def generate_chart(freq_dict, title):\n",
    "        fig, ax = plt.subplots()\n",
    "        x = np.arange(len(freq_dict))\n",
    "        ax.bar(x, [freq_dict.get(score, 0) for score in freq_dict], width=0.5, label=title)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(list(freq_dict.keys()))\n",
    "        ax.set_xlabel('Score')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.set_title(title)\n",
    "        ax.legend()\n",
    "        buf = BytesIO()\n",
    "        plt.savefig(buf, format='png')\n",
    "        buf.seek(0)\n",
    "        chart_data = base64.b64encode(buf.read()).decode('utf-8')\n",
    "        plt.close(fig)\n",
    "        return chart_data\n",
    "\n",
    "    # Generate charts\n",
    "    retrieval_chart = generate_chart(freq_retrieval, 'Retrieval Score Distribution')\n",
    "    exact_match_chart = generate_chart(freq_exact_match, 'Exact Match Score Distribution')\n",
    "\n",
    "    # Generate combined response chart\n",
    "    def generate_response_chart():\n",
    "        fig, ax = plt.subplots()\n",
    "        x = np.arange(len(score_range))\n",
    "        width = 0.3\n",
    "        ax.bar(x - width, [freq_relevance[score] for score in score_range], width, label='Relevance')\n",
    "        ax.bar(x, [freq_similarity[score] for score in score_range], width, label='Similarity')\n",
    "        ax.bar(x + width, [freq_groundedness[score] for score in score_range], width, label='Custom-Groundedness')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(score_range)\n",
    "        ax.set_xlabel('Score')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.set_title('Response Score Distribution')\n",
    "        ax.legend()\n",
    "        buf = BytesIO()\n",
    "        plt.savefig(buf, format='png')\n",
    "        buf.seek(0)\n",
    "        chart_data = base64.b64encode(buf.read()).decode('utf-8')\n",
    "        plt.close(fig)\n",
    "        return chart_data\n",
    "\n",
    "    response_chart = generate_response_chart()\n",
    "\n",
    "    # Generate HTML table\n",
    "    table_html = '<table border=\"1\" style=\"border-collapse: collapse;\"><tr><th>Query</th><th>Response</th><th>Relevance</th><th>Similarity</th><th>Custom-Groundedness</th><th>Exact Match</th></tr>'\n",
    "    for row in rows:\n",
    "        table_html += f\"<tr><td>{row.get('inputs.query', '')}</td><td>{row.get('inputs.response', '')}</td><td>{row.get('outputs.relevance.relevance', '')}</td><td>{row.get('outputs.similarity.similarity', '')}</td><td>{row.get('outputs.custom-groundedness.custom-groundedness', '')}</td><td>{row.get('outputs.exact_match.exact_match_score', '')}</td></tr>\"\n",
    "    table_html += '</table>'\n",
    "\n",
    "    # Generate HTML content\n",
    "    html_content = f\"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>Evaluation Results</title>\n",
    "        <style>\n",
    "            .image-container {{ display: flex; flex-wrap: wrap; justify-content: space-around; }}\n",
    "            .image-container div {{ margin: 10px; text-align: center; }}\n",
    "            img {{ max-width: 100%; height: auto; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Evaluation Results</h1>\n",
    "        <div class=\"image-container\">\n",
    "            <div><h2>Retrieval Score Distribution</h2><img src=\"data:image/png;base64,{retrieval_chart}\"/></div>\n",
    "            <div><h2>Response Score Distribution</h2><img src=\"data:image/png;base64,{response_chart}\"/></div>\n",
    "            <div><h2>Exact Match Score Distribution</h2><img src=\"data:image/png;base64,{exact_match_chart}\"/></div>\n",
    "        </div>\n",
    "        <h2>Results Table</h2>\n",
    "        {table_html}\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    # Save HTML file with timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f'{timestamp}_evaluation_results.html'\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_content)\n",
    "\n",
    "    print(f\"HTML file '{filename}' generated.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_evaluation_report('data/local_upload_cloud_evaluation_output.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"evaluation_result\"](images/evaluation_result_local_upload_cloud1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_agentlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
